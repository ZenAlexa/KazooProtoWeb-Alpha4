# üéØ Roadmap to Commercial Quality (Dubler 2 Level)

## Current Status Assessment (Alpha 6)
**Current Quality: ~25% of Target**
**Phase 1 ÂÆåÊàêÊó•Êúü**: 2025-10-30 ‚úÖ

### Core Issues Identified:
1. **Èü≥È´òÊ£ÄÊµã (Pitch Detection)**:
   - ‚ö†Ô∏è YIN on main thread (46ms latency) ‚Üí **AudioWorklet Êû∂ÊûÑÂ∞±Áª™ (Phase 1 ‚úÖ)**
   - ‚ùå Cannot detect fast pitch changes (slow onset detection)
   - ‚ùå Poor handling of vibrato and slides
   - ‚ùå Basic confidence calculation

2. **Èü≥Ëâ≤ÂêàÊàê (Sound Synthesis)**:
   - ‚ùå Basic oscillators (sawtooth/sine/square) - Âê¨Ëµ∑Êù•Â§™ÂÅá
   - ‚ùå No sample-based synthesis
   - ‚ùå Poor timbre quality
   - ‚ùå Limited expressiveness

3. **Ë°®Áé∞ÂäõÊò†Â∞Ñ (Expression Mapping)**:
   - ‚ö†Ô∏è Basic volume ‚Üí velocity mapping
   - ‚ùå No cents ‚Üí modulation/vibrato mapping
   - ‚ùå No formant ‚Üí brightness mapping
   - ‚ùå No breath noise ‚Üí texture control

4. **UI/UX**:
   - ‚ö†Ô∏è Basic controls and visualization
   - ‚ùå No voice calibration wizard
   - ‚ùå No real-time feedback on input quality
   - ‚ùå Limited visual feedback

---

## üéØ Four-Phase Roadmap

## Phase 1: Ultra-Low Latency Pitch Detection (Target: 5-15ms) ‚úÖ Â∑≤ÂÆåÊàê

**ÂÆåÊàêÊó•Êúü**: 2025-10-30
**Áä∂ÊÄÅ**: Êû∂ÊûÑÂ∞±Áª™ÔºåFeature Flag ÂæÖÂêØÁî®
**ËØ¶ÁªÜÊä•Âëä**: ËßÅ `PHASE1_COMPLETE.md`

**ÂÆåÊàêÁöÑÂ∑•‰Ωú**:
- ‚úÖ ÂàõÂª∫ AudioIO ÊäΩË±°Â±Ç (531 Ë°å)
- ‚úÖ ÂÆûÁé∞ AudioWorklet Á©∫Â§ÑÁêÜÂô® (285 Ë°å)
- ‚úÖ Âª∫Á´ãÊ∂àÊÅØÈÄö‰ø°ÂçèËÆÆ
- ‚úÖ ÈÖçÁΩÆÁÆ°ÁêÜÈõÜ‰∏≠Âåñ (248 Ë°å)
- ‚úÖ Â¢ûÂº∫ÂêØÂä®Êó•Âøó‰∏éÈîôËØØÊèêÁ§∫
- ‚úÖ ÊÄßËÉΩÁõëÊéßÊîØÊåÅ Worklet ÊåáÊ†á
- ‚úÖ Feature Flag + Ëá™Âä®ÂõûÈÄÄÊú∫Âà∂
- ‚úÖ Âü∫Á∫øÊï∞ÊçÆËÆ∞ÂΩï‰∏é Dubler 2 ÂØπÊ†á

**Êû∂ÊûÑÊîπËøõ**:
```
Êóß: Mic ‚Üí ScriptProcessor(2048) ‚Üí ‰∏ªÁ∫øÁ®ã YIN ‚Üí 46ms
Êñ∞: Mic ‚Üí AudioWorklet(128) ‚Üí Áã¨Á´ãÁ∫øÁ®ã ‚Üí 8-15ms (Â∞±Áª™)
```

**‰∏ã‰∏ÄÊ≠• (Phase 2)**:
- Âú® Worklet ‰∏≠ÈõÜÊàê YIN/MPM ÁÆóÊ≥ï
- ÂêØÁî® Feature Flag Âπ∂ÊµãËØïÂÆûÈôÖÂª∂Ëøü
- È™åËØÅÁ≤æÂ∫¶‰øùÊåÅ ‚â• 85%

---

## Phase 1 ËØ¶ÁªÜÂÜÖÂÆπ (Â∑≤ÂÆåÊàê)

### 1.1 Replace YIN with AudioWorklet + Advanced Algorithms
**Current Problem**: YIN on main thread ‚Üí 46ms latency, blocking UI

**Solution Stack**:
```
AudioWorklet (separate thread)
  ‚Üí MPM (McLeod Pitch Method) for fast onset
  ‚Üí SWIPE (Sawtooth Waveform Inspired Pitch Estimator) for accuracy
  ‚Üí Hybrid mode: MPM for transients, SWIPE for sustained notes
```

**Implementation Steps**:
1. Create `pitch-worklet.js` using AudioWorkletProcessor
2. Implement MPM algorithm (better than YIN for low latency)
3. Add SWIPE as fallback for accuracy
4. Ring buffer for zero-copy audio transfer
5. Confidence scoring with multiple metrics:
   - Harmonic clarity (HNR - Harmonics-to-Noise Ratio)
   - Periodicity strength
   - Spectral flatness
   - Voiced/unvoiced detection

**Expected Results**:
- Latency: 46ms ‚Üí 5-10ms (Audio buffer size: 128 samples)
- Accuracy: +30% in fast passages
- CPU usage: -50% (off main thread)

**Technical References**:
- MPM Paper: McLeod & Wyvill (2005) "A Smarter Way to Find Pitch"
- Web Audio API AudioWorklet: https://www.w3.org/TR/webaudio/#AudioWorklet
- Real-time best practices: Buffer size 128-256 samples @ 48kHz

---

### 1.2 Advanced Onset Detection
**Current Problem**: Cannot detect quick note changes (slides vs. discrete notes)

**Solution**:
```javascript
OnsetDetector {
  - Spectral flux analysis
  - Energy envelope tracking
  - Zero-crossing rate
  ‚Üí Classify: [Legato, Staccato, Glissando]
}
```

**Dubler 2 Feature Parity**:
- **IntelliBend**: Anchor to note until intentional bend detected
- **TruBend**: Follow voice precisely (already have this)
- Auto-detect articulation style

---

### 1.3 Frequency Smoothing & Tracking
**Current**: Median filter with 5 samples (too basic)

**Upgrade**:
```javascript
KalmanFilter {
  - Predict next frequency based on trend
  - Correct with new measurement
  - Adaptive noise covariance (trust measurement more when confidence high)
}

+ Adaptive smoothing:
  - Vibrato: More smoothing (reduce jitter)
  - Glissando: Less smoothing (follow precisely)
```

**Expected Results**:
- Vibrato detection: ‚úì (4-8Hz modulation)
- Glissando tracking: ‚úì (smooth pitch bends)
- Jitter reduction: 80%

---

## Phase 2: Multi-Dimensional Expression Mapping

### 2.1 Volume ‚Üí Dynamics
**Current**: `volume * 2` (too simple)

**Dubler 2 Approach**:
```javascript
VelocityMapper {
  // Calibration phase: Learn user's dynamic range
  calibrate(quietestNote, loudestNote) {
    this.minDb = RMStoDb(quietestNote)
    this.maxDb = RMStoDb(loudestNote)
  }

  // Exponential mapping (more natural)
  toVelocity(rms) {
    const db = RMStoDb(rms)
    const normalized = (db - minDb) / (maxDb - minDb)
    return Math.pow(normalized, 0.7) * 127  // 0.7 exponent = natural feel
  }
}
```

**Additional Mapping**:
- Volume ‚Üí Filter cutoff (louder = brighter)
- Volume ‚Üí Release time (softer = longer decay)

---

### 2.2 Cents ‚Üí Modulation (New!)
**Dubler 2 Key Feature**: Use pitch deviation to control effects

**Implementation**:
```javascript
CentsMapper {
  // Cents = pitch deviation from nearest note (-50 to +50)

  mapToVibrato(cents, frequency) {
    // Detect vibrato: Regular oscillation around center pitch
    const isVibrato = detectCyclicPattern(centsHistory, 4-8) // 4-8Hz

    if (isVibrato) {
      return {
        vibratoDepth: Math.abs(cents) / 50 * 0.5,
        vibratoRate: detectedFrequency
      }
    }
  }

  mapToPitchBend(cents) {
    // Intentional bends vs. natural imperfection
    if (Math.abs(cents) > 25 && sustained > 100ms) {
      return cents / 50  // Normalized bend amount
    }
    return 0  // Ignore small deviations
  }

  mapToModulation(cents) {
    // MIDI CC #1 (Modulation Wheel)
    // Map to filter resonance, chorus depth, etc.
    return mapRange(cents, -50, 50, 0, 127)
  }
}
```

**Expected Results**:
- Vibrato automatically applied ‚úì
- Intentional pitch bends captured ‚úì
- Expressive modulation control ‚úì

---

### 2.3 Formant Analysis ‚Üí Brightness (Advanced)
**Goal**: Different vowel sounds control timbre

**Implementation**:
```javascript
FormantAnalyzer {
  // Analyze formants (resonant frequencies in vocal tract)
  // "Ah" = F1:800Hz, F2:1200Hz ‚Üí Warm
  // "Ee" = F1:300Hz, F2:2500Hz ‚Üí Bright

  analyzeFormants(fftData) {
    const peaks = findPeaks(fftData, 5)  // Find 5 strongest peaks
    const f1 = peaks[0]  // First formant
    const f2 = peaks[1]  // Second formant

    // Map to brightness (filter cutoff)
    const brightness = mapRange(f2, 800, 3000, 0.2, 1.0)
    return brightness
  }
}
```

**Use Cases**:
- "Oo" ‚Üí Warm, mellow tone
- "Ah" ‚Üí Balanced
- "Ee" ‚Üí Bright, cutting tone

---

### 2.4 Breath Noise ‚Üí Texture
**Goal**: Air/breathiness controls noisy components (like flute key clicks, sax breath)

**Implementation**:
```javascript
NoiseDetector {
  analyzeSpectralFlatness(fftData) {
    // High flatness = noise (breath)
    // Low flatness = tonal (clear pitch)
    const flatness = spectralFlatness(fftData)

    return {
      noiseLevel: flatness,
      isBreathy: flatness > 0.6
    }
  }
}

// Use in synthesis:
synth.noiseGain = noiseLevel * 0.3  // Add noise texture
```

---

## Phase 3: Professional Sound Engine

### 3.1 Replace Oscillators with Sample-Based Synthesis
**Current Problem**: Sawtooth/sine waves sound NOTHING like real instruments

**Solution**: Multi-sample instrument library

```javascript
SampleInstrument {
  constructor(instrumentName) {
    // Load multiple samples across pitch range
    this.samples = {
      'C2': loadSample('saxophone-C2.wav'),
      'C3': loadSample('saxophone-C3.wav'),
      'C4': loadSample('saxophone-C4.wav'),
      'C5': loadSample('saxophone-C5.wav')
    }

    // Load multiple velocity layers
    this.velocityLayers = {
      'pp': loadSamples('saxophone-pianissimo'),
      'mf': loadSamples('saxophone-mezzoforte'),
      'ff': loadSamples('saxophone-fortissimo')
    }
  }

  playPitch(frequency, velocity) {
    // Find nearest sample
    const baseNote = findNearestSample(frequency)
    const sample = this.samples[baseNote]

    // Pitch shift to exact frequency (preserve formants)
    const playbackRate = frequency / baseNote.frequency

    // Crossfade between velocity layers
    const layer = interpolateVelocityLayers(velocity, this.velocityLayers)

    return {
      sample: sample,
      playbackRate: playbackRate,
      layer: layer
    }
  }
}
```

**Sample Sources**:
1. Free: VSCO 2 Community Edition (high-quality orchestral samples)
2. Free: Freepats (General MIDI samples)
3. Free: Philharmonia Orchestra samples
4. Record your own: MIDI keyboard ‚Üí sample at every C (C1, C2, C3...)

**Expected Results**:
- Sound quality: +500% (from "toy synth" to "real instrument")
- Velocity expression: ‚úì (multiple dynamic layers)
- Natural timbre: ‚úì (real instrument recordings)

---

### 3.2 Intelligent Sample Triggering
**Dubler 2 Approach**: Don't re-trigger on continuous pitch changes

```javascript
IntelligentSampler {
  processPitch(pitchInfo) {
    const isNewNote = detectArticulation(pitchInfo)

    if (isNewNote) {
      // Hard re-trigger
      this.currentSample.stop()
      this.currentSample = this.playSample(pitchInfo)
    } else {
      // Pitch shift existing sample (smooth)
      this.currentSample.playbackRate = calculateRate(pitchInfo.frequency)
    }
  }

  detectArticulation(pitchInfo) {
    const silenceGap = Date.now() - lastPitchTime > 50  // 50ms gap
    const largeJump = Math.abs(pitchInfo.frequency - lastFrequency) > lastFrequency * 0.2  // 20% jump
    const onsetDetected = this.onsetDetector.detect()

    return silenceGap || (largeJump && onsetDetected)
  }
}
```

---

### 3.3 Multi-Layer Synthesis (Hybrid Approach)
**Best of Both Worlds**: Samples + Synthesis

```javascript
HybridSynth {
  constructor() {
    this.sampler = new SampleInstrument()  // Body/resonance
    this.synth = new Tone.MonoSynth()      // Expressiveness
  }

  playNote(frequency, velocity, expression) {
    // Layer 1: Sample (60% mix) - authentic timbre
    const sample = this.sampler.play(frequency, velocity)
    sample.volume.value = -5  // dB

    // Layer 2: Synth (40% mix) - continuous control
    this.synth.frequency.value = frequency
    this.synth.volume.value = -8  // dB

    // Blend based on articulation
    if (expression.isLegato) {
      // More synth for smooth tracking
      sample.volume.value = -10
      this.synth.volume.value = -3
    }
  }
}
```

---

### 3.4 Advanced Effects Chain
**Current**: Basic vibrato + filter + reverb

**Professional Chain**:
```javascript
EffectsChain {
  // 1. EQ (shape timbre)
  eq: new Tone.EQ3({
    low: 0,      // Controlled by formants
    mid: 0,      // Controlled by volume
    high: 0      // Controlled by brightness
  })

  // 2. Compressor (even out dynamics)
  compressor: new Tone.Compressor({
    threshold: -20,
    ratio: 4,
    attack: 0.003,
    release: 0.1
  })

  // 3. Chorus (thicken sound)
  chorus: new Tone.Chorus({
    frequency: 2,
    delayTime: 3.5,
    depth: 0.3,
    wet: 0.2
  })

  // 4. Reverb (space)
  reverb: new Tone.Reverb({
    decay: 2.5,
    preDelay: 0.01,
    wet: 0.25
  })

  // 5. Limiter (prevent clipping)
  limiter: new Tone.Limiter(-1)
}
```

---

## Phase 4: UI/UX Overhaul

### 4.1 Voice Calibration Wizard (Critical!)
**Dubler 2's Secret**: Personalized to each user's voice

```javascript
CalibrationWizard {
  steps: [
    {
      title: "ÊâæÂà∞‰Ω†ÁöÑÊúÄ‰ΩéÈü≥",
      instruction: "ËØ∑Â∞ΩÂèØËÉΩ‰ΩéÂú∞ÂìºÂî± 'Ahhhh'ÔºåÊåÅÁª≠2Áßí",
      analyze: (recording) => {
        this.userRange.min = recording.lowestStableFrequency
      }
    },
    {
      title: "ÊâæÂà∞‰Ω†ÁöÑÊúÄÈ´òÈü≥",
      instruction: "ËØ∑Â∞ΩÂèØËÉΩÈ´òÂú∞ÂìºÂî± 'Ahhhh'ÔºåÊåÅÁª≠2Áßí",
      analyze: (recording) => {
        this.userRange.max = recording.highestStableFrequency
      }
    },
    {
      title: "ÊµãËØïÂä®ÊÄÅËåÉÂõ¥",
      instruction: "‰ªéÊúÄËΩªÂà∞ÊúÄÂìçÔºåÊÖ¢ÊÖ¢Â¢ûÂä†Èü≥Èáè",
      analyze: (recording) => {
        this.dynamicRange = {
          quietest: recording.minRMS,
          loudest: recording.maxRMS
        }
      }
    },
    {
      title: "È¢§Èü≥ÊµãËØï",
      instruction: "Âî±‰∏Ä‰∏™Èü≥ÔºåÂä†‰∏äÈ¢§Èü≥",
      analyze: (recording) => {
        this.vibratoStyle = detectVibratoCharacteristics(recording)
      }
    }
  ],

  applyProfile() {
    pitchDetector.setRange(this.userRange.min, this.userRange.max)
    velocityMapper.calibrate(this.dynamicRange.quietest, this.dynamicRange.loudest)
    vibratoDetector.setThreshold(this.vibratoStyle.depth)
  }
}
```

**Impact**: +200% accuracy for that specific user

---

### 4.2 Real-Time Input Quality Indicators
**Problem**: User doesn't know if they're being detected correctly

**Solution**: Multi-meter dashboard
```html
<div class="input-quality-panel">
  <!-- Pitch Confidence -->
  <div class="meter">
    <label>Èü≥È´òÁΩÆ‰ø°Â∫¶</label>
    <div class="meter-bar" id="confidenceMeter">
      <div class="fill" style="width: 85%; background: lime"></div>
    </div>
    <span>85% - Excellent</span>
  </div>

  <!-- Volume Level -->
  <div class="meter">
    <label>Èü≥Èáè</label>
    <div class="meter-bar" id="volumeMeter">
      <div class="fill" style="width: 60%; background: lime"></div>
    </div>
    <span>-12 dB - Good</span>
  </div>

  <!-- Latency Monitor -->
  <div class="latency-display">
    <span class="value">8ms</span>
    <span class="label">ÊÄªÂª∂Ëøü</span>
    <span class="status">‚úì ‰ºòÁßÄ</span>
  </div>

  <!-- Voiced/Unvoiced Indicator -->
  <div class="voice-status">
    <span class="indicator voiced">üéµ Ê∏ÖÊô∞‰∫∫Â£∞</span>
    <!-- OR -->
    <span class="indicator unvoiced">üí® Ê∞îÂ£∞/ÊùÇÈü≥</span>
  </div>
</div>
```

---

### 4.3 Advanced Visualizations
**Current**: Basic frequency display

**Professional Suite**:

1. **Pitch Tracker (Melodyne-style)**
```javascript
PitchTrackDisplay {
  // Real-time scrolling piano roll
  // Shows: detected pitch vs. target notes
  // Color-coded by confidence

  render(pitchInfo) {
    const note = frequencyToMidiNote(pitchInfo.frequency)
    const cents = pitchInfo.cents

    drawNote({
      position: note + (cents / 100),
      color: confidenceToColor(pitchInfo.confidence),
      thickness: velocityToThickness(pitchInfo.velocity)
    })
  }
}
```

2. **Spectrogram (Formant Visualization)**
```javascript
// Show frequency content over time
// User can SEE their formants changing
// Useful for learning vowel control
```

3. **Expression Meter**
```javascript
ExpressionDisplay {
  // Real-time bars showing:
  - Vibrato depth
  - Pitch bend amount
  - Brightness (formant)
  - Breathiness
  - Dynamic level
}
```

---

### 4.4 Performance Mode vs. Practice Mode
**Two UI Layouts**:

**Practice Mode** (default):
- All meters visible
- Pitch accuracy feedback
- Note names displayed
- Slow update rate (easier to read)

**Performance Mode**:
- Minimal UI (only instrument selector)
- Hide distracting metrics
- Fastest update rate
- Full-screen option

---

## üìä Implementation Timeline & Priority

### Sprint 1 (Week 1-2): Foundation
**Priority: Critical**
- [ ] Implement AudioWorklet with MPM algorithm
- [ ] Replace ScriptProcessorNode with AudioWorkletNode
- [ ] Benchmark latency reduction
- [ ] Add Kalman filter for frequency tracking

**Expected Outcome**: Latency 46ms ‚Üí 8-12ms

---

### Sprint 2 (Week 3-4): Expression Mapping
**Priority: High**
- [ ] Implement cents ‚Üí modulation mapping
- [ ] Add formant analyzer (FFT-based)
- [ ] Create calibration wizard (basic version)
- [ ] Improve velocity curve

**Expected Outcome**: Capture vibrato, brightness, dynamics

---

### Sprint 3 (Week 5-6): Sound Quality
**Priority: High**
- [ ] Integrate sample library (start with 1 instrument)
- [ ] Build SampleInstrument class
- [ ] Add velocity layer crossfading
- [ ] Implement intelligent sample triggering

**Expected Outcome**: Sound quality 10x improvement

---

### Sprint 4 (Week 7-8): Polish & UX
**Priority: Medium**
- [ ] Build input quality indicators
- [ ] Add pitch tracker visualization
- [ ] Implement performance vs. practice mode
- [ ] Add effects presets per instrument

**Expected Outcome**: Professional user experience

---

## üî¨ Technical Specifications

### Target Metrics
| Metric | Current | Target | Dubler 2 |
|--------|---------|--------|----------|
| End-to-end latency | 46ms | 8-12ms | 5-10ms |
| Pitch accuracy | 85% | 95%+ | 98% |
| Vibrato detection | No | Yes | Yes |
| Dynamic range | 20dB | 60dB | 80dB |
| CPU usage (1 voice) | 15% | 5% | <5% |
| Buffer size | 2048 | 128-256 | 128 |
| Sample rate | 44.1kHz | 48kHz | 48kHz |

---

### Audio Processing Pipeline (Final Architecture)

```
Microphone Input (48kHz, 128 buffer)
  ‚Üì
AudioWorkletNode (separate thread)
  ‚Üì
MPM Pitch Detector (5ms)
  ‚Üì
Kalman Filter (smooth)
  ‚Üì
[Parallel Analysis]
  ‚îú‚Üí Onset Detector ‚Üí Articulation
  ‚îú‚Üí Formant Analyzer ‚Üí Brightness
  ‚îú‚Üí Spectral Flatness ‚Üí Breathiness
  ‚îî‚Üí Cents Tracker ‚Üí Vibrato/Bend
  ‚Üì
Expression Mapper
  ‚Üì
Hybrid Synth Engine
  ‚îú‚Üí Sample Layer (60%)
  ‚îî‚Üí Synth Layer (40%)
  ‚Üì
Effects Chain (EQ ‚Üí Compressor ‚Üí Chorus ‚Üí Reverb)
  ‚Üì
Limiter ‚Üí Output
```

**Total Latency Breakdown**:
- Audio input buffer: 128 samples @ 48kHz = 2.67ms
- MPM detection: ~2-3ms
- Processing overhead: ~1ms
- Output buffer: 128 samples = 2.67ms
- **Total: ~8-10ms** ‚úì

---

## üéì Learning Resources

### Pitch Detection Algorithms
1. **MPM Paper**: "A Smarter Way to Find Pitch" - McLeod & Wyvill (2005)
   - https://www.cs.otago.ac.nz/research/publications/oucs-2005-08.pdf
2. **SWIPE Paper**: "A Sawtooth Waveform Inspired Pitch Estimator" - Camacho (2007)
3. **YIN Paper**: "YIN, a fundamental frequency estimator" - De Cheveign√© & Kawahara (2002)

### Web Audio API
1. Official Spec: https://www.w3.org/TR/webaudio/
2. AudioWorklet Guide: https://developers.google.com/web/updates/2017/12/audio-worklet
3. Performance Tips: https://padenot.github.io/web-audio-perf/

### Synthesis Techniques
1. Free Sample Libraries:
   - VSCO 2 Community Edition
   - Freepats General MIDI
   - Philharmonia Orchestra (free for non-commercial)
2. Formant Analysis: Praat (phonetics software) documentation
3. Effects Design: "Designing Audio Effect Plugins in C++" by Will Pirkle

---

## üí° Quick Wins (Do These First!)

### Week 1 Immediate Improvements:
1. **Switch to 128 buffer size** (if browser supports):
   ```javascript
   bufferSize: 128  // Was 2048
   sampleRate: 48000  // Was 44100
   ```
   **Expected**: Latency 46ms ‚Üí 25ms

2. **Add onset detection** (detect note starts):
   ```javascript
   const energyChange = currentEnergy / previousEnergy
   if (energyChange > 2.0 && timeSinceLastOnset > 100) {
     isNewNote = true
   }
   ```
   **Expected**: Better articulation

3. **Implement cents ‚Üí vibrato** (low-hanging fruit):
   ```javascript
   if (Math.abs(cents) > 10 && isOscillating(centsHistory)) {
     vibrato.depth.value = Math.abs(cents) / 50
   }
   ```
   **Expected**: Automatic expressive vibrato

---

## üöÄ Success Criteria

### Phase 1 Complete When:
- [ ] Latency < 15ms consistently
- [ ] No UI blocking during pitch detection
- [ ] Vibrato detected and visualized

### Phase 2 Complete When:
- [ ] User can control brightness with vowel sounds
- [ ] Vibrato automatically applied
- [ ] Calibration wizard saves user profile

### Phase 3 Complete When:
- [ ] Saxophone sounds like a saxophone (blind test pass)
- [ ] Can play smooth legato passages
- [ ] Effects sound professional

### Phase 4 Complete When:
- [ ] New users can calibrate in < 2 minutes
- [ ] Input quality always visible
- [ ] Performance mode is distraction-free

---

## üì¶ Recommended Libraries

### Pitch Detection
- `pitchy` (MPM implementation): https://github.com/ianprime0509/pitchy
- `aubio.js` (compiled to WASM): https://github.com/qiuxiang/aubio.js

### Audio Processing
- `essentia.js` (music analysis): https://mtg.github.io/essentia.js/
- `meyda` (audio feature extraction): https://meyda.js.org/

### Sample Management
- `tone.js` Sampler (already using)
- `soundfont-player` (MIDI soundfonts): https://github.com/danigb/soundfont-player

### Visualization
- `d3.js` (custom visualizations)
- `wavesurfer.js` (waveform display)
- `WebGL` (high-performance spectrogram)

---

## üéØ Final Target: Dubler 2 Feature Parity

### Must-Have Features:
- [x] Real-time voice to instrument
- [ ] IntelliBend (smart pitch bend)
- [ ] Auto key detection
- [ ] Velocity sensitivity
- [ ] CC control from voice timbre
- [ ] MIDI output (future: Web MIDI API)
- [ ] Recording and playback
- [ ] Preset management

### Beyond Dubler 2 (Your Unique Features):
- [ ] Browser-based (no download!)
- [ ] Shareable performances (URLs)
- [ ] Collaborative jamming (WebRTC)
- [ ] Visual learning tools
- [ ] Mobile-optimized (iOS Safari support)

---

## üìù Next Immediate Actions

1. **Read MPM paper** (30 mins)
2. **Set up AudioWorklet** (2 hours)
3. **Implement basic MPM** (1 day)
4. **Benchmark latency** (30 mins)
5. **Add onset detection** (4 hours)

**Total Sprint 1 Estimate**: 3-4 days full-time

---

Generated: 2025-10-30
Version: Alpha 5 ‚Üí Beta 1 Roadmap
Target Completion: Beta 1 in 8 weeks (realistic timeline)
